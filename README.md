# Bowling Dice Game Reinforcement Learning

The idea for this project came while I was staying at an Airbnb near Cincinnati. The house had a big tin of old sports teaser games which use dice rolling and pegs to play games like football, baseball, soccer, etc. Most of the games are complete luck where the dice roll directly determines the outcome such as "Throw a 40 yard pass" or "Interception!" However, the rules for the bowling game leave some room for interpretation and strategy. They read "For 1 or more players. Begin by putting one peg in every hole. Player rolls dice. Remove those pegs indicated by the numbers on the dice. The numbers may be used separately or added together. A player keeps rolling until the numbers can no longer be used. Now it is the next player's turn." The scoring is done as follows: a Strike (10 pins) is 25 points, a Spare (9 pins) is 20 points, 8 pins is 8 points, 7 pins is 7 points, etc. I chose to interpret these rules as meaning you could take any combination of the value of the first die, the second die, and the sum (i.e. the first, the second, the sum, the first and second, the first and sum, the second and sum, or the first, second, and sum). This means that you have a choice to make on every roll. I would consider each score to be part of a frame and a whole game to be 10 frames.

![Bowling](/assets/bowling_image.jpg)

## Strategy

I wanted to come up with the best possible strategy for this game. At first, it may seem that the best strategy would be to knock down as many pins as possible after every roll since knocking down pins directly correlates to points. However, knocking down a lot of pins early in the game without much thought makes it more likely to reach a point where no numbers can be used before getting a spare or a strike. This strategy resulted in an average score of about 80 for a game (which is pretty bad considering the average score for choosing completely randomly is 75). So, I quickly abandoned that strategy and came up with a new idea.

My new strategy was to knock down only 1 pin at a time to maximize my chance of staying alive in the game. The pin that I knock down would by the least likely of the three options to roll. So the priority would be 10 (3/36), 9 (4/36), 8 (5/36), 7 (6/36), 1 (12/36), 2 (13/36), 3 (14/36), 4 (15/36), 5 (16/36), 6 (17/36). This way, the pins still up at the end would be more likely to roll and the likelihood of the game ending would be lower. This strategy ended up with an average score of about 95 (a whole lot better than before, but still not too good as far as bowling scores go).

My next strategy was pretty much the same as the previous one, but with a small yet significant tweak. I would pick the least likely pin (just like before) until I got a roll where knocking down more pins would give me a spare or strike, in which case I would knock down as many pins as possible. The increase in score from from 8 to 9 pins is 12 points! So, I decided that it was worth it to knock down those pins even if it meant there was a low likelihood of knocking down the last pin. This strategy resulted in an average game score of about 102 (still maybe not quite the best, but finally getting close to my average score in real bowling). After this strategy, I couldn't come up with anything better, so I turned to the computer to try and improve upon it.

## Implementation

To implement the game, I decided reinforcement learning was probably the way to go. I ended up using DQN, which combines Q-learning with neural networks, with PyTorch by following a [tutorial](https://www.youtube.com/watch?v=6pJBPPrDO40) of how to build an AI to play Snake (parts 3 and 4). I eventually found that the code in the Q-learning piece of the tutorial was not working properly for my purposes, so I followed a [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) for that piece of the code.

To run the code, simply clone the repository and run `plot.py`. This will start training a new model from scratch. The blue line shows individual game scores while the orange line shows the mean score. The model will save every so often in `./model` based on `SAVE_FREQUENCY` in `plot.py`. The name of the file will be `{num_games_trained}_{mean_score}_{legal_moves_percentage}.pth`. If you would like to load a previously trained model, change `LOAD_FILE_NAME` to the name of the model you would like to load and `EPSILON` to 0. `EPSILON` helps the model explore new actions rather than exploit what it already knows. When the model just begins training, it is good to have `EPSILON` as a somewhat high number and let it decay as the model learns more so that the model can get a good sense of the action space. However, when loading an already trained model, it is better to have the model continue to exploit what it already knows rather than exploring by taking random actions.

You can run two models against each other using `bowling.py`. At the bottom of the file, change the filenames in the initialization of each `Agent` to the models you would like to compete. You can also use any class with a `make_decision(state, legal_decisions)` method as an agent. I've implemented a `Jon_Agent` which uses the best strategy described above to make its decisions.

The reward function I used takes the difference in the score before and after an action. So if it makes a move which brings the number of knocked down pins from 3 to 4, the reward would be 1. If it makes a move which brings the number of knocked down pins from 7 to 9, the reward would be 13. If it makes an illegal move, the reward would be 0. I worry that this reward function has too much preference towards the actions which knock down only 1 pin since these are the actions that are most likely to be legal and get a reward. However, I was unable to find a reward function that worked better than this one.

## Results

So far, the computer is unable to beat the strategy I developed on my own. It has essentially learned the untweaked version which only knocked down 1 pin at a time based on the likelihood that the pin would be rolled, but has not been able to figure out the tweak to get spares and strikes more consistently. The best model's average score is around 96.
